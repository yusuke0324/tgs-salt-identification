{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/alexanderliao/u-net-bn-aug-strat-dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#なぜこのkernelはスコアが高い？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
    "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
    "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    #return res\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "    #return img[:img_size_ori, :img_size_ori]\n",
    "    \n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def bce_logdice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = weight * (logit_y_pred * (1. - y_true) + \n",
    "                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    loss = 1. - K.sum(score)\n",
    "    return loss\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd\n",
    "    averaged_mask = K.pool2d(\n",
    "            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "train_df = pd.read_csv(\"./data/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"./data/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "\n",
    "train_df[\"images\"] = [np.array(load_img(\"./data/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"masks\"] = [np.array(load_img(\"./data/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]\n",
    "\n",
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n",
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    train_df.coverage.values,\n",
    "    train_df.z.values,\n",
    "    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(m, dim, acti, bn, res, do=0):\n",
    "\tn = Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
    "\tn = BatchNormalization()(n) if bn else n\n",
    "\tn = Dropout(do)(n) if do else n\n",
    "\tn = Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
    "\tn = BatchNormalization()(n) if bn else n\n",
    "\treturn Concatenate()([m, n]) if res else n\n",
    "\n",
    "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
    "\tif depth > 0:\n",
    "\t\tn = conv_block(m, dim, acti, bn, res)\n",
    "\t\tm = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
    "\t\tm = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
    "\t\tif up:\n",
    "\t\t\tm = UpSampling2D()(m)\n",
    "\t\t\tm = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
    "\t\telse:\n",
    "\t\t\tm = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "\t\tn = Concatenate()([n, m])\n",
    "\t\tm = conv_block(n, dim, acti, bn, res)\n",
    "\telse:\n",
    "\t\tm = conv_block(m, dim, acti, bn, res, do)\n",
    "\treturn m\n",
    "\n",
    "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "\t\t dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n",
    "\ti = Input(shape=img_shape)\n",
    "\to = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
    "\to = Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
    "\treturn Model(inputs=i, outputs=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet((img_size_target,img_size_target,1),start_ch=16,depth=5,batchnorm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=bce_dice_loss, optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6400 samples, validate on 800 samples\n",
      "Epoch 1/200\n",
      "6400/6400 [==============================] - 68s 11ms/step - loss: 0.8416 - acc: 0.8278 - val_loss: 0.7856 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78560, saving model to ./keras.model\n",
      "Epoch 2/200\n",
      "6400/6400 [==============================] - 64s 10ms/step - loss: 0.6040 - acc: 0.8843 - val_loss: 0.6217 - val_acc: 0.8898\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78560 to 0.62174, saving model to ./keras.model\n",
      "Epoch 3/200\n",
      "6400/6400 [==============================] - 64s 10ms/step - loss: 0.5084 - acc: 0.9006 - val_loss: 0.4987 - val_acc: 0.9099\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62174 to 0.49874, saving model to ./keras.model\n",
      "Epoch 4/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.4741 - acc: 0.9045 - val_loss: 0.5580 - val_acc: 0.8899\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.4420 - acc: 0.9099 - val_loss: 0.5208 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.4086 - acc: 0.9164 - val_loss: 0.4256 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.49874 to 0.42558, saving model to ./keras.model\n",
      "Epoch 7/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.3879 - acc: 0.9205 - val_loss: 1.6060 - val_acc: 0.7825\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.3846 - acc: 0.9209 - val_loss: 0.4268 - val_acc: 0.9150\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.3600 - acc: 0.9261 - val_loss: 0.3833 - val_acc: 0.9227\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.42558 to 0.38327, saving model to ./keras.model\n",
      "Epoch 10/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.3410 - acc: 0.9285 - val_loss: 0.4441 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.3349 - acc: 0.9301 - val_loss: 0.3414 - val_acc: 0.9301\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.38327 to 0.34139, saving model to ./keras.model\n",
      "Epoch 12/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.3282 - acc: 0.9313 - val_loss: 0.3694 - val_acc: 0.9326\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.3156 - acc: 0.9339 - val_loss: 0.3778 - val_acc: 0.9280\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.3042 - acc: 0.9364 - val_loss: 0.3356 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.34139 to 0.33557, saving model to ./keras.model\n",
      "Epoch 15/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.2848 - acc: 0.9391 - val_loss: 0.3151 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.33557 to 0.31507, saving model to ./keras.model\n",
      "Epoch 16/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.2672 - acc: 0.9434 - val_loss: 0.3263 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.2655 - acc: 0.9435 - val_loss: 0.3381 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.2585 - acc: 0.9442 - val_loss: 0.4403 - val_acc: 0.9159\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.2494 - acc: 0.9450 - val_loss: 0.3420 - val_acc: 0.9277\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.2425 - acc: 0.9470 - val_loss: 0.3067 - val_acc: 0.9374\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.31507 to 0.30668, saving model to ./keras.model\n",
      "Epoch 21/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.2243 - acc: 0.9499 - val_loss: 0.3078 - val_acc: 0.9347\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.2169 - acc: 0.9516 - val_loss: 0.3765 - val_acc: 0.9318\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.2086 - acc: 0.9534 - val_loss: 0.3192 - val_acc: 0.9352\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1933 - acc: 0.9556 - val_loss: 0.3293 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1883 - acc: 0.9560 - val_loss: 0.3409 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1883 - acc: 0.9566 - val_loss: 0.3139 - val_acc: 0.9388\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 27/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1465 - acc: 0.9642 - val_loss: 0.2820 - val_acc: 0.9417\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.30668 to 0.28201, saving model to ./keras.model\n",
      "Epoch 28/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1313 - acc: 0.9666 - val_loss: 0.2795 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.28201 to 0.27945, saving model to ./keras.model\n",
      "Epoch 29/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1242 - acc: 0.9677 - val_loss: 0.2702 - val_acc: 0.9438\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.27945 to 0.27015, saving model to ./keras.model\n",
      "Epoch 30/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1222 - acc: 0.9680 - val_loss: 0.2808 - val_acc: 0.9420\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1169 - acc: 0.9689 - val_loss: 0.2942 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1107 - acc: 0.9699 - val_loss: 0.2926 - val_acc: 0.9419\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1122 - acc: 0.9697 - val_loss: 0.2925 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1080 - acc: 0.9704 - val_loss: 0.2859 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1071 - acc: 0.9702 - val_loss: 0.2888 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 36/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1049 - acc: 0.9707 - val_loss: 0.2882 - val_acc: 0.9418\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1036 - acc: 0.9710 - val_loss: 0.2889 - val_acc: 0.9421\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1026 - acc: 0.9710 - val_loss: 0.2862 - val_acc: 0.9429\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/200\n",
      "6400/6400 [==============================] - 63s 10ms/step - loss: 0.1022 - acc: 0.9712 - val_loss: 0.2880 - val_acc: 0.9429\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 00039: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=10, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src: https://www.kaggle.com/aglotero/another-iou-metric\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./keras.model\")\n",
    "preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\n",
    "preds_valid = np.array([downsample(x) for x in preds_valid])\n",
    "y_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:01<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "thresholds = np.linspace(0, 1, 50)\n",
    "ious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm(thresholds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ffa0d535080>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEZCAYAAACAZ8KHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYFNXZ9/HvsCmggKOICoLIcgsqCgpqxAVx3zAxCopL\n1IgbxiwqMcY14iNxw4gazMMbjUnEx6gRjRHUxBiMUVxRlltURDYFHAaRdZip949TA0Uzw3TPdPds\nv8919dXdVXWqTvVSd52tqiCKIkRERNLVpLYzICIi9YsCh4iIZESBQ0REMqLAISIiGVHgEBGRjChw\niIhIRhQ4UpjZTWb2WB6208XMysws4+/AzI4ws/lbmf97M7u1ZjmsXWZ2iZndU9v5EKmrzKyFmc0y\nsx3zve1m+d5gbTOzlUD54JXWwDqgNJ52STw9X4NbarKdWh2AY2ZlQHd3/6yK5c4Hfujuh6VMnwtc\n5O7/qCBNc+B6YEBi2v7A/wK9gJnxOj+oZJsfAZ0Tk1oCL7j7kPhP9iywF9A0Xtc17v6fCtbzCjAI\naObuZYnpw4Ab420sBn7g7q/H834IjAI6AFPjfVwcz/sxcCWwE7ASeCLedlk8fz/gfqAP8A3wsLvf\nFs+7DvgFm773ZkALYGd3L9raPsfpy4BV8bwImOjuI+J55wMTgNVAQTz/ZHd/LZ5/BfADYF/gz+5+\nYcrnNBgYB+wOvAlc4O5fxPOuBs4HugBLgYfc/a5E2i7A74GDgHnAle7+SjzvSOA38Xo3AK/F8xel\nflcVydZvJp5/FHAn0D3ejzHu/rtE+q5xXo8A1gL/z91/XtXnZ2YHAb8CDoj38VXgKnf/Mp5/E+G/\nsJZN300fd//c3deb2QTgOuDqdD6TbGl0JQ53397d27h7G8IP9aTEtMczWZeZNc1NLuuFTAJXpkFu\nCDAr8edpDvwV+APQLn5+1swqPPFx933Kv+P4e54P/F88+1vgIsIBdwfg18BzqSU/MzubcHCOUqYf\nA/wPcL67bwccDnwWzzsSGA2cAhQCnwPJ39SzwIHu3hbYB9gf+FFi/p+BV929HXAkcLmZnRzv0/+k\n/HbHxMsWpbHPsOmAU76OESkf23/i6eXzX0vMW0g4uE1I/azjQPwU4eBWCLxDCIhJ5xK+txOAkWZ2\nZmLe43GaQuCXwF8SZ9AzgBPi72k34BPgodQ8VCSbv5k4zdOEoNcWGAbcY2b7Jrb1EvAysDPQCfhj\nYvWVfn7ADsB4QmDtQvh9/j5lmYkp383niXmPA+fHecibRlfiSFEQP1JtY2aPAt8lBJfz3f1d2Him\n/BAwHOhpZq0JZ5f3Ew4iK4Gx7n5/vHx/4EGgJ+GM7k/uXn52UACcY2a/IpzhjHX32+N0LQgHtTMI\nf/ongWvdvSQ1s2bWl3Bm1R34O5UcqON1fgUc6u4z42k7AV8QzrYi4BFgIFAGfOTuR2zls0uuN628\npukE4F+J90cCTd39N/H7++Mz2aOAKVtbkZkdAexI+OPj7usAj+cVEPazHeHAtSye3oZQojgPeCNl\nlTcDt7r7tHh9ixPzTgKedPfZ8Xp+BSw0s67uPtfd5yaWbRpvu3tiWhdC8MDdPzOzqcDewPMV7Np5\nwE3p7HOsgGqeKLr7X+P19gc6psz+HuF38nS8zM3AMjPr6e4fJ0sXwMdm9ixwKPB/ZtYT6AscE38v\nT5vZVcDphNLW0kTaJoTPq1ua2T6SLP1mCL+N7YmDgbu/bWazgN7Ah4TSxEJ3vy+xmo/KX2zt83P3\nF1O2PY5Q6kiLuy80syLgYODf6aarqUZX4kjTKYQ/cFvgOeCBlPnDCAe3doQD5XPAe8CuwGDgqvjM\nFOA+QkBoS/jR/1/Kug4FegBHAzeamcXTf0moqukD7Be//mVqRuMzjWeARwk/8CcJf7wtuPt6wtnh\nWYnJZxLOXJcBPyOcae1IOHP6RUXrqUBaec3AvsQH99jewPSUZT6Ip1flPOApd1+TnGhmHxCK/38F\nfhfvf7nbCcH+q5Q0TYADgZ3NbI6ZfWFm95vZNpVsu/z/tU9iHWeZ2QpCdUcfwtlmubGEs8dm8e/g\nYMKZ7GbM7HCgPZsHhir3GfiXmS0ys7/EVURJfc1siZnNNrNfZtD2tjfhuwDA3VcTSgaVfTeHsemg\n2hv4zN1XJeZv9r2a2e5mtpxw0vVTQkkr3Xxl5Tfj7ksIZ/YXmlkTMzuEcKJVfqA+GJhnZi+Y2VIz\n+4eZ7VPhmqt2BKGklXSKmS0zsw/N7NIK0swm/O/yRoGjYlPdfbK7R8BjhD940n3uvig+S+oP7OTu\no929NC5G/i8huACUAN3NbEd3X+3ubyXWEwE3u/t6d59O+GGX/wDOBm5x96/d/WvgFkKRP9UhhDr4\n38TbfwqYtpV9e5zNA8fZwJ8Sed0V6Bqv6/WtrCcp3bymqx2h5FZuO2BFyjLfEM4CK2VmLYHvs2XR\nH3ffL05/NvB6Is2BwHcIJchUHYDmhMB8KKGqqS+bguSLwBlmtk+87RsJZ8mtEtt9PD6J6AH8ls2D\n09/i/K4h1MlPKC/ppjgP+Et8kE53nw8H9iC07SwGnk8Eh38B+7j7zvG+nQVcU8F2K5L2d2NmtxBK\nPo+km9bd58dVVTsSPuePs52vlDxW9vlNJHyf6wif1/WJtpZOwFBC4N8VeIGtVIttZdt9gBvYvL3i\nCUIbTXtgBOHkcmhK0pWE/0zeKHBU7MvE69XAtilnYAsSr7sAHc2sKH4sJzRW7RzPvxAwYLaZvWlm\nJ6VsK3ngWE34wUOo0/0iMW9ePC3VroQ6VFKWrcw/gZZm1j8+69yPcNYNobrpU2CKmX1iZqO2sp6k\nreV1A+Fgm6o5IVBVZDmb/8G/BdqkLNOWzYNLRU4Hvnb3CovwccB+ArjOzPaNq64eIDRORmyqjit/\nLj+D/427L4nbF+4BTozX9wqhKutpQrvHZ3Eek7+X8m1/SggODwGY2Q6EwHMzsA2hQfj41DPM+MB2\nBpsOvmnts7tPdfcN7v4NcBUhiPSK533u7vPi1zOAWwkHz3Sk9d2Y2UjgHODERBVm2t+ruxezqZ0i\nneNW1n4zcenvCeAcd29OKLWMMrMT4kXWEE42p8Sf8V2EQNcrjXyWb6M7IeBc6YmOGu4+292/dPfI\n3d8g1GCkfjfbA8XpbisbFDiqJ9mGMJ9Q3C6MHzu4e1t3PwXCAcLdz3b39oQD81/iP39VFhGCUrku\n8bRUi9my3rlzBcsR56eMUF12NuHM8vnyqgJ3X+XuV7t7N+BU4KdmNqiGeS1vP9nIzFoRAmtlAW46\noU2o3Ay2LPX1YcsifarzCAebqjQH9iQcaA4EnjCzxcBbhKAx38wOjQ9eqUFgs/Ykd3/I3Xu6+66E\nANKMRH13Jdslft7g7n9y97L4bHYicVBK+B7hwPYaFUtnn1MD4taWqcoMQskLgLjNrxuJ78bMLgSu\nBY5KaROaAewZpym3H5V/r80JZ96pAaGyfGXrN7MPMNvdXwZw9zmE0mF54JhODXo5xidwLxFK7X+u\nYvHkCU25XiSqC/OhsTeOp2trf6K3gJVmdi2hO14JoTqgZdyINhyYHNehryB88eVdO7e23seBX5rZ\n2/H7GwjVZqneADaY2ZWEs9dTCW0MW3RzTVn3XwmNwdeXT4xLQ7Pjs+GVhNJCWYVrSD+vbwJr49LL\nWMJv7n+AaR532azAC8Cl8XIQGgtL430cH88r29o+mlknQlfaS1KmHxTn4S1CA/VVhCD2pruvMLNd\nE4t3jpfrR9xwTqjCuNLMJhM+n58Q2riI2zq6u/sMM+sMPExo31oRz78ImOTuS82sN/BzQmcGCFUw\nBRa6+j5BqBYbCrySsmuVBoat7HNvwkH3Q0K12WhCAJwVzz8eeNfdl5jZXoQqoScS6ZvG6ZsCzeL9\n3ODupYT2tV+b2XcJ39tNwPvu/nGcdni8vSPLSzXl3H2Omb0P3GRmNxA6F+xDaIcjXucMYA6hC/M9\ncT6L4/k3AUe4+1EVfByvkqXfDKH9sruZDXL3f5pZN+Bk4I54/h8JJ1lHxdu9itCGVf75Vvr5mVlH\nwnd8vye69ybydCrwmrsXm9mAeN2jEvN3I/TM+m9l+5ULjb3Eke5ZQlTJ6/Iz+JMJZ11zgSXA79h0\nVnQ8MMPMvgHuBYbGbSMVbT/5/jbgbcLZzAfx69GpGYuL/d8DLgC+JlRjPLW1nYnbWVYRqrn+npjV\nA3jZwliX14EH3P1fFawi7bzGDfInEf6QCwgNp7sQGuUr8xyhhmCXxD6eRhgPsJxw8Bzi7hsIC55t\nZh+mrOMc4HXfvCcThGqgBwiBYAHh+znR466/cRXUkrhBdGm8n0vKt0XoVvk24UA/g9CV9PZ43rbA\nn+PP77+Ez/DGxLYPBT6M5z8fP66Pt7uS8D3+FCgC3o0/z43feXyQGETlJYrK9rkDIRCsIHz+uxPG\naZTG8wcD0xP5+gubgjaEQLKacMAaHr8uz/cyQvXO7XG+D2RT+17551UITDOzlWb2jZk9mJg/jNBO\nuDze19PjdjIIJekXCW0THxAC9fcSaXcn0T6VlM3fjIexShcBv4k7NvyT0HtuQjz/4zjt+PgzOAU4\nNfGbqfTzi9fbFbg5/mxWxseK5OfzSTztEeB2d0929R0OPOrV78FYLQW5vpFTfDYzlhCkJrj7mJT5\nbQgRuzMhIt/t7o+kk1YaLgsD6Xq7+09rOy9SN5nZu8Bgd19e23mpDRa6wb8PHO6b9wrMuZwGjrgR\n62PCGc0iQm+fYR73c4+XuQ5o4+7XWRhT4IQzpLKq0oqISP7luqpqADDH3efFRamJhFHBSRGbetBs\nT2j425BmWhERybNcB46OhF5H5RawZQ+gcUBvM1tEqMe8KoO0IiKSZ3WhV9VxwHvuflTcW+GleCBM\nWuIeCv0J3VJLq1hcRERCe/KuhN6N66paOFWuA8dCNu/D34ktB6tdQNyDw90/tXAtqL3STAshaOTt\nGi0iIg3IYYSrOGck14FjGqH/cxdCiWAYm1/uAsIgsKOB182sA2Hg12eEroNVpSWex5/+9Cd22WWX\nnOyEiEhD8uWXXzJ8+HCIj5+ZymngiAe4jCRcjbK8S+0sM7sEiNz9YcIYgEfMrPyCZNfGl3KgorQV\nbKYUYJdddqFTp0653B0RkYamWtX7OW/j8HDZYEuZNj7xejGhnSOttCIiUrsa+8hxEalL/vY3KE65\nXl9xcZgudYYCh4jUHYceCtdfvyl4FBeH94ceWrv5ks0ocIhI3dGuHWW/Gs03V17Pgqmfs+an17P6\n+tGUtcnr7SakCnVhHIdIvVZUBK+8AlOmwNSpsO220L79lo8ddoAogrIyKC0Nj/LXzZrBTjttvnzr\n1lCQuH5yFMGqVbBiRTgRLy6G7baDPfaAtm1rbfdr7Kuv4M03Nz3efrsdvVpewxt/7ErfdnPxie1Y\nuxZatoRWrcLneMABMGAAHHQQ9O0b5kn+KHCIZGjDhnCAmzIFJk+GmTPhsMPguONg5Mgwf+lSWLYs\nPC9dCp9/DsuXh0DQtCk0aRKey1+XlGy+/NKlIVC0bw/Nm4dgsWIFtGgB7dqFR9u2sHJlWHfTpiGA\ndOkSHp07h22tXh2CzerVm16vXQtt2kBhIey4Y3guf92uXdhG8+ZbPqJoU9BKBq8VK8K6k4Ew+XrD\nhrB/JSWwfv2m12vXwkcfhfTlQeAnP4GDrJid7r0TrpnLe3feCaNDiWPNmrCdpUth2rTwHfzxjzBr\nFvTuHdIfeCDsu29436pVFV+kVJsCh0gVFi4MB6m33grP77wD3brBscfC7beH6vdtKrvreA2UHyTX\nrw9n2W3bhgN4qigKQenzz2HevPD4Ir7TSevWId2uu4bXrVqFvK5cGUpKRUXw4YebXhcXb35wT74u\nKAjrKg9ayQDWqlUIXi1abBkUmzXbFHySQalFC9hrL+jRIywHbGrTGD06rHz0aLj+epqMHk3rdu1o\n3ToE09694fzzQ5I1a+Ddd8N38/LLMHYsfPwxdOoUgsg++4Tnvn3D91aQ7i2qpFI5v6x6rpnZHsDc\nV155ReM4pMbWrw8HoalT4Y03wsFo7dpwNlv+6N8/nKFLDvztbyESt0u0aRQXw+uvw0mpd12uXEkJ\nzJkTSjQffRSC4zvvhO/ykEM2Pfr3b5wlkwULFjB48GCAru7+eabpFTikUVuxIgSIqVPD4+23wxnw\nwIHhwHLQQbDnnjpLbSgWLAjf93/+E54//BD23HMcbdvO46ST7qRHj/D9d+sW2o+yaeHChQwePJiZ\nM2fSpElm/ZLeeustrrnmGv71r4rvq3bdddexyy67cNVVV1U4P1VNA4eqqqRBW78+nHHOn7/5Y8GC\n8Lx0aTjrHDgQrrsODj64fjc01xdHHXUUX3/9NU2bNqV58+b07duXW265hQ4dOtR4vaNHj+aQQw6p\ncP6iRW8xbtymA/CaNXDTTTBnTgFFRfCnP4WSymefhUJPjx7wne/A0UeHgtC229YoexTU4AykJmmz\nTYFDGpxVq+DFF+GZZ+CFF6Bjx9Bw3KkT7L57qPPefffwvnPnUNcusSxVFaVj/PjxHHzwwaxfv56b\nb76ZX/3qV4wbNy6r20gVRdFmB+CWLcNvAODXv960XFlZaNtyh9degxtuCKWTgw4q5eijm3L00aHN\npGnTnGa3ztI4DmkQvv4aHnkEhgwJDcHjx4fjX3n99nPPwUMPwS9+AeeeC0ceCd27K2hsIY8D8Mqr\nyVu0aMFxxx3HJ598snHe+vXrGTNmDIMGDWLgwIHcfPPNrF+/HoDly5dz6aWX0r9/fw466CDOOecc\nAK699loWL17MZZddRr9+/ZgwYcJm21uzZg0jRoxgyZIl9O3bl379+rF06dKN2xs1ahT9+vXjlFNO\nYdasGey+eyhpTJ16FBdc8DsOP/xUFi/uy8KFZZx77hK6dfsRvXsfwoEHHs2ttz7Guvji5NOnT+f0\n00/ngAMOYODAgYwZs+mO11EUMWnSJAYNGsQhhxzCb3/72832efTo0Rx22GEcfvjh3H777ZSUVHwr\n8ZkzZ/K9732PAw44gJ/85CesW5fxldFrJoqiev3o2bPnHj179ozmz58fScNRWhpFn34aRc89F0Vj\nxkTR+edH0SGHRNG++0ZRjx5RtPvuUdS+fRS1aRNFLVpEUatWUfTd70bRY49FUVFRbee+nlu+PIou\nvzyK5s4Nz8uXZ30TgwYNiv7zn/9EURRFq1evjkaNGhX9/Oc/3zh/9OjR0WWXXRZ988030apVq6JL\nL700uueee6IoiqK77747uummm6LS0tJow4YN0dtvv73Zet94441Kt/vmm29GRxxxxGbT7r///qhP\nnz7Ra6+9FpWVlUV33313dOaZZ262ztNOOy368ssvo3Xr1kVlZWXRd7/73WjMmAejiRM3RD/84fzI\n7OiosHBqdNhhUXTwwUOj2257Nlq3LuzbBx98EEVRFC1YsCAys+iGG26I1q1bF82aNSvaZ599ok8/\n/TSKoigaO3ZsNHTo0KioqCgqKiqKhg4dGt13331b5Hv9+vXRoEGDokcffTTasGFD9OKLL0Z77713\nNHbs2LQ///nz50c9e/aMevbsuUdUjeOuqqokr6IodBVdsCB0//z6601dQYuKwliGTz4JVQQ77RS6\nXfbqFdogLroojD/YdtvNH9tss6kbqGRBu3ZwzTXQtSvMnbt5tVUWXXHFFTRr1ozVq1dTWFi4WQnh\nySefZNKkSWy/fbir9IgRI7j66qv5yU9+QrNmzVi6dCkLFiygc+fOHHDAAZutN6pGh58DDjiAww47\nDIAhQ4bwhz/8YbP555133sb2l+nTp1NcXMy1114GwNChnXj44TOYM+dvHHPModx6azOefPIL7r13\nOUOG7MCZZ/ahV6+wnoKCAkaOHEmLFi3Ya6+92GuvvZg9ezZ77rknzz//PDfeeCM77LADACNHjuSm\nm27iRz/60WZ5ef/999mwYQPnnXceAMcddxy///3vM97nmlDgkJxZswZmzIAPPtj0mD491CvvsceW\nA9D23ju879Yt9O/ffvsqNyG5UFwMd94ZgkY8AC8XwePBBx/k4IMPJooiXn75Zc455xxeeOEFCgoK\nWLNmDaeffvrGZcvKyjYGhIsuuohx48Zx4YUXUlBQwBlnnMGIESNqlJeddtpp4+ttt92WdevWUVZW\ntrH3U/JePwsXLuSrr75iwIABQAhUZWVl9O/fn2OPhb32up377ruPf//7BD7+eHd+8YsrmDfvSE44\nIZw4tWu3+bZWr14NwJIlS9htt902ztttt91YsmTJFnldunTpFp0IOnbM7121FTgka0pLw4jev/89\nNErPmBF6pey3X3icemp4bt++tnMqlapkAF4ugkd5ICgoKOCYY47hxhtv5J133uGYY46hZcuWPP/8\n8+y8885bpGvdujWjRo1i1KhRfPLJJ5x33nn06dOHgw8+uMqeR9nombTrrrvSqVMnJk+eXOH8zp07\nc/fddwMwefJkrrnmR7z++lv8+c9hEGmvXjBhQmhnS9p5551ZuHAh3bp1A2DRokUV7n/79u356quv\nNpu2aNEiOnfuvMWyuaLCvdTI0qXhsg9nnw0dOsDFF4eSxq9/HUYzf/AB/OEP8LOfhYZGBY067vXX\nNw8S5cHj9ddzutmXX36ZlStX0q1bt42liNtvv52ioiIAvvrqK6ZODXc4ffXVV/kiHhrfunVrmjZt\nStO4e9NOO+3EggULKt3OjjvuSHFxMd9+++1W87O16q4+ffrQunVrfve737Fu3TpKS0uZM2cOH374\nIQCTJk3amO/tt9+egoICunZtwogRUFAQcc89cM45cMUV4WSr3EknncRDDz1EUVERRUVFPPjggwwZ\nMmSL7e+///40a9aMxx57jA0bNjBlypSN284XlTgaudJSWLcujHdo2bLyS2dEURj3kKx2+uADWLIE\njjoKTjgBxowJ3VylHquoy227dlnvigtw2WWX0aRJEwoKCthtt90YM2bMxrPtq6++mgceeIAzzzyT\n4uJiOnTowFlnncXAgQP5/PPPufXWW1m+fDlt27Zl+PDh9O/fHwhtIbfddht33nknl112GRdccMFm\n29xzzz056aSTGDx4MFEU8bdK7vORLJmkllKaNGnC+PHjueOOOxg8eDAlJSV07dp14+C7f//739xx\nxx2sXbuWjh07cu+999Ii7r5XUFDAKafA4YfDT38K06YV0Lt3WO/ll1/OqlWrOPXUUykoKOCEE07g\n0ksv3SJvzZs35/777+eXv/wlY8eO5YgjjuDYY4+txjdQfRo53kCtWxeu1zNzZnjMmhUexcXhsgvr\n1oXnkpLQwNy8eSgpFBSEtoU2bcLz9tuHaTNnhqBSXu1U/jAL1yISkcy98AJccgmcfHIopeerXU8j\nxxu51GvyfPRRaFv44ovQKaZXr9Az6bvfDSOjd9xxy95I5SdUURQCysqVmx7ffBNKJb17QwXVrSJS\nAyeeGMYZ/exn4UKMd90Fp51W90/Gcp49MzseGEtoT5ng7mNS5l8NDAcioDnQC9jJ3YvN7HNgBVAG\nlLj7gFznt65bvx6efDKcqXz00ZZXAR02LDxXZ3BbQcGmoKK2CJH8aNcuNJZPmQK/+hVcdRX88Ieh\nvbCuVqLkNHCYWRNgHDAYWARMM7Nn3X12+TLufhdwV7z8ycCP3b38psNlwJHuvjyX+awPli0Lo6Ef\nfDB0VR0+PNy7QPcdEGkYjj02PD78MPzX+/QJbSGXXQbHHFO3xinlOisDgDnuPs/dS4CJwJbdBDY5\nC3g88b6ARt7za8YMGDEidGv97LPQ1fWVV+DCC8NNaxQ0RBqWffeFceNCdfNJJ4Uq5h49wtV864pc\nH5Q7AvMT7xfE07ZgZi2B44GnEpMj4CUzm2ZmF+csl3XQq6+Gs4+jjw7FVfdQnO3Tp7ZzJiL5sN12\nobrqnXfCRRavuCJcfLEuqEtn86cAUxPVVACHuns/4ETgCjMbWDtZy59XXw0Dgy6+GM46K9zV7cYb\n1TAt0lgVFIS7HbZoAY8/XvXy+ZDrwLEQSA5n7BRPq8gwNq+mwt0Xx89LgWcIVV8NUjJgXHhh6Dp7\nwQW5uSWpiNQvBQWhu+7114du9LUt14FjGtDdzLqYWQtCcJiUupCZtQWOAJ5NTGtlZtvFr1sDxwIf\n5Ti/eVdRwDjvvLrfHU9E8uvww0NV9YMP1nZOchw43L0UGAlMAWYAE919lpldYmbJq5KdBkx29zWJ\naR2AqWb2HvBf4Dl3n5LL/ObTe+/BcceFbncKGCKSjjvuCI/ltdzPVCPH82zu3NDQ9cor4fnii8Oo\nbRGRdPzwh2Eg75gxVS9bmZqOHK9LjeMN2rJlYdzFgQeGwXkffwyXX66gISKZueUW+N//DdeOqy0K\nHDlWVhYatfbaK1weZOZMuPlm3WtCRKqnY0e49NLQ27K2qEY9hzZsCFVR7mHwTo8etZ0jEWkIrr0W\nevYMN0arjbFdKnHkyLp1MHQoLFoEL72koCEi2dO2beia+/Of1872FThyYNUqOOWU0Pd60iRo3bq2\ncyQiDc2ll4bajH/8I//bVuDIsuLicKmQjh1h4kQN4BOR3GjRItyc8dpr838pEgWOLFqyBAYNCj2n\nJkzQmAwRya0zzwz3y6nk9uc5o8CRJV98AYcdBqeeCmPH1q1LIItIw9SkSbh69iOP5Hm7+d1cw1RS\nEu65ffHFoY91yi2KRURyZuhQePHF/I4mV+DIggcfDJc+/9nPajsnItLYFBaGyxc98UT+tqnAUUPL\nlsFtt8G996qkISK14/zz4dFH87c9BY4auvHGcN+M3r1rOyci0lgdd1y4Dp57franwFED06fDU0+F\nS4iIiNSWZs3gnHPyV+pQ4KimKIIf/ziUOAoLazs3ItLYnX8+PPZY6J6bawoc1fTXv4ZxG5dcUts5\nERGBffeF9u3zM5JcgaMa1q6Fq68O4zU0yE9E6oof/CA/1VUKHNUwdmyI7kcfXds5ERHZ5Kyz4Pnn\n4ZtvcrsdBY4MLV4Md90VHiIidUn79nDkkfDkk7ndjgJHhn7xC7joonAXPxGRuiYf1VU5r6E3s+OB\nsYQgNcFeqZu9AAAWBUlEQVTdx6TMvxoYDkRAc6AXsJO7F1eVNt+mTQsXE5s9uzZzISJSuRNPDNev\n+vRT6NYtN9vIaYnDzJoA44DjgL2Bs8xsr+Qy7n6Xu/d1937AdcCrcdCoMm0+lZbCyJHhMsZt2tRW\nLkREtq5FCxg2DP7wh9xtI9dVVQOAOe4+z91LgInAkK0sfxbweDXT5tSDD0LLlqEYKCJSl/3gByFw\n5Oo+HbkOHB2B+Yn3C+JpWzCzlsDxwFOZps21+fPDVW/Hj9f1qESk7uvbF7bbDv7979ysvy41jp8C\nTHX34trOSFIUwRVXwI9+BGa1nRsRkaoVFISR5Lm6T0euA8dCoHPifad4WkWGsamaKtO0OfP00/DJ\nJzBqVL63LCJSfcOHwzPPwPr12V93rntVTQO6m1kXYDEhOJyVupCZtQWOIPSuyihtLhUXh5LGE0/o\n3uEiUr/sums4bn39dXidTTktcbh7KTASmALMACa6+ywzu8TMRiQWPQ2Y7O5rqkqby/ymuu46OPlk\nGDgwn1sVEcmOwkIoKsr+enM+jsPdXwQsZdr4lPePAlsMWakobb68/jpMmgQzZtTG1kVEai5XgaMu\nNY7XGevXhwE0994L7drVdm5ERKpHgSOPfv1r6NoVzjijtnMiIlJ99baqqr75+ONw9dt33tGYDRGp\n31TiyJM//hEuvBC6dKntnIiI1IwCR568+y4cckht50JEpOYUOPLk3XehX7/azoWISM0pcOTB4sWw\nbh107lz1siIidZ0CRx68914obahRXEQaAgWOPFA1lYg0JAocefDOOwocItJwKHDkgUocItKQtG0L\n334LGzZkd70KHLFly2D58tzdo1dEJN+aNAmXTSrO8l2OFDhi770X7prVRJ+IiDQguaiu0mEypmoq\nEWmIFDhy6N134YADajsXIiLZpcCRQypxiEhDpMCRIytWhFHjViu3jBIRyR0Fjhx5/33o0weaNq3t\nnIiIZJcCR46omkpEGqpcBI6c38jJzI4HxhKC1AR3H1PBMkcC9wLNgaXuPiie/jmwAigDStx9QC7y\n+O67MGhQLtYsIlK7Cgth2rTsrjOnJQ4zawKMA44D9gbOMrO9UpZpCzwAnOzu+wDJG7aWAUe6e99c\nBQ1QiUNEGq76WFU1AJjj7vPcvQSYCAxJWeZs4Cl3Xwjg7ssS8wpyncdVq2DuXOjdO5dbERGpHfUx\ncHQE5ifeL4inJfUECs3sn2Y2zczOTcyLgJfi6RfnIoPTp4eg0aJFLtYuIlK76mPgSEczoB9wAnA8\ncIOZdY/nHeru/YATgSvMbGC2N65qKhFpyOpj4FgIJO+n1ymelrQAmOzua939a+A1YD8Ad18cPy8F\nniFUfWWVAoeINGQ77BAuclhWlr115jpwTAO6m1kXM2sBDAMmpSzzLDDQzJqaWSvgIGCWmbUys+0A\nzKw1cCzwUbYzqMAhIg1Zs2bQujV880321pnTwOHupcBIYAowA5jo7rPM7BIzGxEvMxuYDEwH/gs8\n7O4zgQ7AVDN7L57+nLtPyWb+1q0Dd9h332yuVUSkbtlhh+xWV+V8HIe7vwhYyrTxKe/vAu5KmTYX\n2D+XefvoI+jeHVq2zOVWRERqV3k7x557Zmd9daFxvNaomkpEGoNsN5ArcChwiEgDp8CRRe+8o8Ah\nIg2fAkeWlJSENo79c9qKIiJS+xQ4smTWLOjSBbbbrrZzIiKSWwocWaL2DRFpLBQ4skSBQ0QaCwWO\nLFHgEJHGorAQli/P3voaZeAoLYUPPoC+fWs7JyIiuacSRxbMmQM77wzt2tV2TkREck+BIws+/RR6\n9qztXIiI5Ef5taqiKDvra5SBY/ly2HHH2s6FiEh+tGwJTZvC6tXZWV+jDBxFRSECi4g0Ftmsrmq0\ngaOwsLZzISKSPwocNaTAISKNjQJHDSlwiEhjo8BRQwocItLYKHDUkAKHiDQ22QwcW711rJl9L2VS\nBCwD3nf3ldnJQv4pcIhIY5O3wAGcUtH2gT5mdpG7/6OqDZjZ8cBYQulmgruPqWCZI4F7gebAUncf\nlG7a6li+XN1xRaRxKSwMg5+zYauBw90vqGi6mXUB/g84aGvpzawJMA4YDCwCppnZs+4+O7FMW+AB\n4Fh3X2hmO6WbtjrKyhQ4RKTxqfU2DnefRygdVGUAMMfd57l7CTARGJKyzNnAU+6+MF73sgzSZmzl\nSmjVCpqnk3sRkQYin1VVFTIzA9alsWhHYH7i/QJCQEjqCTQ3s38C2wG/cffH0kybMbVviEhjlM/G\n8ecIDeKbbR/YFTgnO1mgGdAPOApoDbxhZm9kad1bUOAQkcYonyWOu1LeR8DXhCqk9WmsfyHQOfG+\nUzwtaQGwzN3XAmvN7DVgvzTTZkyBQ0Qao7wFDnf/V/lrM+sA9AfaAEuBJWmsfxrQPW5MXwwMA85K\nWeZZ4H4zawpsQ2hwvwfwNNJmTIFDRBqj1q2hpATWrq35utJqHDezM4G3gDOAM4E3zez7VaVz91Jg\nJDAFmAFMdPdZZnaJmY2Il5kNTAamA/8FHnb3mZWlzXQHU6lHlYg0RgUF4diXjVvIpts4fj3Q392X\nAJhZe+Bl4C9VJXT3FwFLmTY+5f1dbFktVmHamlKJQ0Qaq/LqqrZta7aedLvjNikPGrGvM0hbpyhw\niEhjla12jnRLHC+a2WTg8fj9UOCFmm8+/4qKoFev2s6FiEj+lQeOrl1rtp60Sg3ufg3wMNAnfjzs\n7qNqtunaoRKHiDRW+S5x4O5PAU/VfJO1S4FDRBqrwsI8NI6b2Uq2HAAIUABE7t6m5lnILwUOEWms\n8lLicPfta76JukWBQ0Qaq8JCmDGj5uuplz2jakLjOESkscpWiaNRBY41ayCKoGXL2s6JiEj+KXBU\nQ3k1VUFBbedERCT/FDiqQe0bItKYKXBUgwKHiDRmChzVoMAhIo1Z27bw7bdQWlqz9ShwiIg0Ek2a\nhODxzTc1XE92slM/qCuuiDR2hYVQXFyzdTSqwKESh4g0dgocGVLgEJHGrrAQVqyo2ToUOEREGhGV\nODKkwCEijZ0CR4YUOESksctG4Ej7fhzVZWbHA2MJQWqCu49JmX8E8CzwWTzpaXe/LZ73ObACKANK\n3H1ATfKiwCEijV1hISxaVLN15DRwmFkTYBwwGFgETDOzZ919dsqir7n7qRWsogw40t2zcOsRBQ4R\nkfpQVTUAmOPu89y9BJgIDKlgucouO1hAlvJYUgKrV8P2De4OIyIi6asPgaMjMD/xfkE8LdUhZva+\nmf3NzHonpkfAS2Y2zcwurklGiouhXbswclJEpLGqD4EjHe8And19f0K11l8T8w51937AicAVZjaw\nuhtRNZWISP0Yx7EQ6Jx43ymetpG7f+vuq+PXfweam1lh/H5x/LwUeIZQ9VUtChwiIvWjxDEN6G5m\nXcysBTAMmJRcwMw6JF4PAArcvcjMWpnZdvH01sCxwEfVzYgCh4hIuF5fTS9ymNNeVe5eamYjgSls\n6o47y8wuASJ3fxj4vpldBpQAa4ChcfIOwDNmFsX5/JO7T6luXhQ4RESgWTNo1aqG68hOVirn7i8C\nljJtfOL1A8ADFaSbC+yfrXwocIiIBG3a1Cx9XWgczwtdUl1EJGjXrmbpG03gUIlDRCRQ4EiTAoeI\nSKDAkSYFDhGRoG3bmqVX4BARaWRU4kiTAoeISKDAkSYFDhGRQIEjDWVlYYi9uuOKiKiNIy0rV4aR\nks1yPtxRRKTuU4kjDaqmEhHZRIEjDQocIiKbdOlSs/QKHCIijcw229QsvQKHiIhkRIFDREQyosAh\nIiIZaTCBI4oqn6dLqouIZE+DCRwLF1Y+TyUOEZHsaTCB45NPKp+nwCEikj0KHCIikpGcX4TDzI4H\nxhKC1AR3H5My/wjgWeCzeNLT7n5bOmmT5sypPA8KHCIi2ZPTwGFmTYBxwGBgETDNzJ5199kpi77m\n7qdWMy2gwCEiki+5rqoaAMxx93nuXgJMBIZUsFxBDdICoaqqop5VUaTAISKSTbkOHB2B+Yn3C+Jp\nqQ4xs/fN7G9m1jvDtAC0aAGLF285fc2a8NyyZSbZFhGRytSFxvF3gM7uvj+hauqv1VlJ9+4wc+aW\n05cvV2lDRCSbch04FgKdE+87xdM2cvdv3X11/PrvQHMzK0wnbVKPHhUHDlVTiYhkV64DxzSgu5l1\nMbMWwDBgUnIBM+uQeD0AKHD3onTSJvXoAbNmbTldgUNEJLtyGjjcvRQYCUwBZgAT3X2WmV1iZiPi\nxb5vZh+Z2XuErrdDt5a2sm1VVlWlwCEikl05H8fh7i8CljJtfOL1A8AD6aatjAKHiEh+1IXG8axo\n3x42bIClSzefrsAhIpJdDSZwFBRAr15btnMocIiIZFeDCRwAvXtvWV2lS6qLiGRXgw8cKnGIiGRX\ngwscqqoSEcmtBhU4evVSiUNEJNcaVODYfXdYsSI8yilwiIhkV4MKHE2abNmzSoFDRCS7GlTggM0b\nyEtKYPVqaNOmdvMkItKQNLjAkWznWL4c2rULYzxERCQ7GlzgSJY4dEl1EZHsa5CBo7yNQ+0bIiLZ\n1+ACR9eu8OWXsGqVAoeISC40uMDRtCn07AmzZytwiIjkQoMLHLCpukqBQ0Qk+3J+P47aUN5A3ry5\nAoeISLY1yBJHeZdclThERLKvwZY4Zs2C7bbTJdVFRLIt54HDzI4n3Eu8CTDB3cdUslx/4D/AUHd/\nOp72ObACKANK3H1AOtvs3h2++AI6dVKJQ0Qk23IaOMysCTAOGAwsAqaZ2bPuPruC5e4AJqesogw4\n0t2XZ7LdFi1Ct9xp0xQ4RESyLddtHAOAOe4+z91LgInAkAqWuxL4C7AkZXoB1cxjr16wcqUCh4hI\ntuU6cHQE5ifeL4inbWRmuwGnuftDhECRFAEvmdk0M7s4kw337h2eFThERLKrLvSqGguMSrxPBo9D\n3b0fcCJwhZkNTHel5YFDjeMiItmV68CxEOiceN8pnpZ0IDDRzOYC3wceMLNTAdx9cfy8FHiGUPWV\nll69wuXUmzXIfmMiIrUn14fVaUB3M+sCLAaGAWclF3D3Pctfm9nvgefcfZKZtQKauPu3ZtYaOBa4\nJd0N77MPjB+fjV0QEZGknJY43L0UGAlMAWYAE919lpldYmYjKkgSJV53AKaa2XvAfwkBZUq6227W\nDIYNq0HmRUSkQjmvyHH3FwFLmVZhWcDdL0y8ngvsn9vciYhIpupC47iIiNQjChwiIpIRBQ4REcmI\nAoeIiGREgUNERDKiwCEiIhlR4BARkYwocIiISEYUOEREJCMKHCIikhEFDhERyYgCh4iIZESBQ0RE\nMqLAISIiGVHgEBGRjChwiIhIRhQ4REQkIwocIiKSEQUOERHJSM7vOW5mxwNjCUFqgruPqWS5/sB/\ngKHu/nQmaUVEJH9yWuIwsybAOOA4YG/gLDPbq5Ll7gAmZ5pWRETyK9dVVQOAOe4+z91LgInAkAqW\nuxL4C7CkGmlFRCSPch04OgLzE+8XxNM2MrPdgNPc/SGgIJO0IiKSfzlv40jDWGBUDdI3Bfjyyy+z\nkxsRkQYucbxsWp30uQ4cC4HOifed4mlJBwITzawA2Ak4wcw2pJkWYFeA4cOHZyvPIiKNxa7Ap5km\nynXgmAZ0N7MuwGJgGHBWcgF337P8tZn9HnjO3SeZWdOq0ia2cVi8TGlO9kJEpGFpSgga06qTOKeB\nw91LzWwkMIVNXWpnmdklQOTuD6ckiapKW8E21gFTc7YTIiINU8YljXIFURRVvZSIiEisLjSOpyWd\nwYBm9hvgBGAV8AN3fz+/ucyPqj4LMzubTR0OVgKXufuH+c1lftRkgGlDk+Z/5EjgXqA5sNTdB+U1\nk3mSxn+kDfBHQjtqU+Bud38k3/nMNTObAJwMfOXufSpZJuPjZr245Eg6gwHN7ASgm7v3AC4Bfpv3\njOZBmgMjPwMOd/f9gNuA3+U3l/lRkwGmDU2a/5G2wAPAye6+D3BG3jOaB2n+Lq4AZrj7/sAg4G4z\nqzcn0hn4PeFzqFB1j5v1InCQ3mDAIcAfANz9TaCtmXXIbzbzosrPwt3/6+4r4rf/peGOf6nJANOG\nJp3P4mzgKXdfCODuy/Kcx3xJ57OIgO3j19sDX7v7hjzmMS/cfSqwfCuLVOu4WV8CRzqDAVOXWVjB\nMg1BpgMjfwj8Pac5qj01GWDa0KTzu+gJFJrZP81smpmdm7fc5Vc6n8U4oLeZLQI+AK7KU97qmmod\nN+tL4JBqMLNBwAXUbIBlfZc6wLQhB4+qNAP6EeqzjwduMLPutZulWnMc8J677wb0BR4ws+1qOU/1\nRn0JHOkMBlwI7F7FMg1BWgMjzawP8DBwqrtvrahan2UywHQu8H3CAeLUPOUvn9L5LBYAk919rbt/\nDbwG7Jen/OVTOp/FBcDTAO7+KTAXaIwXUa3WcbO+NAZVOZAQmERo8HrCzA4Git39q/xmMy+q/CzM\nrDPwFHBu/KdoqKo9wDSvucyPdP4jzwL3x4NrtwEOAu7Jay7zI53PYh5wNPB6XKffk9CppCEqoPKS\ndrWOm/WixOHupUD5YMAZwMTygYRmNiJe5gVgrpl9AowHLq+1DOdQOp8FcANQCDxoZu+Z2Vu1lN2c\nSvOzSGqwg5bS/I/MJvQsm07oNPGwu8+srTznSpq/i9uA75jZdOAl4Fp3L6qdHOeOmf2Z0A29p5l9\nYWYXZOO4qQGAIiKSkXpR4hARkbpDgUNERDKiwCEiIhlR4BARkYwocIiISEYUOEREJCP1ZQCgSNaZ\nWSHwCmF8x66EO0guAboCC+MryGZze0cAV7v7KRmk+SfwM3d/N2X6+cCB7n5lNvMokg6VOKTRcvci\nd+/r7v2Ah4B74tf7A2VVpY9HYGcqmwOnNAhLaoVKHCJB6iUZmpnZw8B3CNd4GuLu6+ISwPvAQODP\nZvYY4R4G5df7+bG7vxGXLsYSDu4RcHg8f3szexLYB3jb3c8FMLPBwJ2EmwpNI9x8qySZITO7APg5\n4TLZ04G1Wdt7kQyoxCFSsR7A/XF11Qrg9MS85u7e393vBe4jlFQOIlxEcUK8zM+Ay+MSzGHAmnj6\n/sCPgN5ANzP7jpltQ7jhzhnxzbeaA5clM2NmuwA3A4cQglbvLO+vSNoUOEQq9lnidrvvAHsk5j2R\neH00MM7M3iNcMG47M2sFvA7ca2ZXAju4e3nV11vuvtjdI0LJZQ/A4u2VX5DyUTaVUModBPwzrl7b\nkJIHkbxSVZVIxdYlXpcC2yber0q8LgAOSq1WAsaY2fPASYQrsB5byXrL/4Pp3CekMd9LROoQlThE\nKpbuQXoKibvHmdl+8fOe7j7D3X9NaLPY2r0eHOhiZuWXgD8XeDVlmTeBw81sBzNrTgO9X7jUDwoc\nIhWrrMdS6vSrgAPN7AMz+wi4JJ7+YzP70Mw+ANZT8e17IwB3X0e4sdBf4uVLCZe4Ti7zJaGN47/A\nv4EGdzl0qT90WXUREcmIShwiIpIRBQ4REcmIAoeIiGREgUNERDKiwCEiIhlR4BARkYwocIiISEYU\nOEREJCP/H3xYBqRsbPydAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffa0d49e4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "threshold_best_index = np.argmax(ious[9:-10]) + 9\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]\n",
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source https://www.kaggle.com/bguberfain/unet-with-depth\n",
    "def RLenc(img, order='F', format=True):\n",
    "    \"\"\"\n",
    "    img is binary mask image, shape (r,c)\n",
    "    order is down-then-right, i.e. Fortran\n",
    "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
    "\n",
    "    returns run length as an array or string (if format is True)\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if format:\n",
    "        z = ''\n",
    "\n",
    "        for rr in runs:\n",
    "            z += '{} {} '.format(rr[0], rr[1])\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18000/18000 [00:32<00:00, 548.74it/s]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array([upsample(np.array(load_img(\"./data/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18000/18000 [06:42<00:00, 44.69it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_test = model.predict(x_test)\n",
    "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm(test_df.index.values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('./submission/submission_anothermodel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
